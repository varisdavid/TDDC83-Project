In this project we are striving to use Test Driven Development as our methodology for the project. This means that after development is done tests are ran to ensure functionality of the application. If tests are not cleared the application is then changed to fit tests and then refactored.



\subsection{Test Levels}
We have a focus on Integration Tests, Regression tests and some User tests. We use OpenEHR for our backend and we won't be testing its functionality with Unit-tests, this could be implemented later on, but is not seen as a requirement. To improve traceability, every requirement is put in gitlabs boards, acting as an RTM. There every requirement is labeled as, untested, test developed, in development. This is put in place to make sure that development is done via our test cases made from requirements. Each test case is added as a comment to the requirement to give the developer an idea how the final product should work.



Most of our tests are designed as e2e tests. End 2 End testing is designed from how the end user would use the application. Instead of testing code in the application, an e2e test simulate human interaction with the website to trigger the different funtions. This is because 1. most of our code is written in css and html with functionality in the website itself, like pressing buttons. 2. Our testing team is very small, we only have 2 testers and 1 testleader, e2e tests makes sure that the core functionality is there and emulates the path of a potential user. There are negatives to the e2e tests since they don't cover all functionality as good as for example extensive unit testing does. We try to counteract that to do edge cases on specific functionality like the filtering function. 
\subsubsection{Testing process for integration tests}
The test defining process goes as follows for integration testing:

Every Requirement can have many tests.

\begin{enumerate}
    \item Pick functional requirement that is developed and ready to be tested. This is done in our Gitlab issue boards. Move the issue mapped to the requirement that is going to be tested to the label test case and assign yourself to the task.
    \item Using the Requirement try to map how a user would interact with the website by looking at input, action and output. 
    \item Write down the steps that you take manually and fill this into the test description field in the Gitlab boards issue mapped to the specific requirement.
    \item Map these steps in a selenium script.
    \item Move the issue in gitlab to the "ready to Develop" board.
    \item Wait for issue to be moved to "testing" on the gitlab board.
    \subitem If the test goes through, the gitlab issue will be moved to closed.
    \subitem If the test does not go through, the gitlab issue will be moved back to "developing", a log of the error-message will be added as a comment to the issue. 
\end{enumerate}




\subsubsection{Black box Unit testing}
Can be skipped, talk with Sana.

Black box testing for every function is tested manually via black box testing. Every current function/button/textfield is mapped in DOCUMENT. Testing of these components will both be done with the expected inputs but also trying prohibited inputs in text fields etc. An example of this could be letters in the date input or letters in min or max age input. By doing this black box Unit testing we can ensure branch coverage where every function is tested. 


\subsection{User Tests}
To get a better understanding on how an end-user would navigate the platform we have performed user tests. A Lo-Fi user test as well as a regular user test.

\subsubsection{Lo-fi test}
This user test was done on a lo-fi prototype together with nurses at Region Östergötland. The Lo-Fi tests were made through our lo-fi prototype and consisted of scenarios where the user should perform a instruction. The test implemented think aloud where the user spoke their thoughts at every action and impression. The tests were also recorded, both the actions the user made on the computer aswell as the spoken audio. These thoughts were then decoded and made into requirements after the test. At the end of the test a modified version of a SUS-test was performed to see how the users thought about the System Usability. The SUS-test was modified in only having 6 questions rather than 10, since some questions were not covered by our Lo-Fi test.

Full test report can be found in Appendix A.
\subsubsection{Final user test}
This user test was done with our final prototype together with two workers at Region Östergötland as well as members from our Project group. The test is structured the same way as the Lo-Fi test and goes through the same scenarios as the Lo-Fi test. The users will then rat the test test using SUS-tests. The users interaction with the website will also be recorded and Lostness of the website will be derived from this recording. 

Since we had limited testers available from RÖ and according to SUS a reliable test must have atleast 6 test subject, we decided to add some project members to the test group. This might affect the testing result since our members are familiar with how our application works, but we would rather have a larger group of testers and slightly too high results than unreliable ones due to shortage of testers. 

Full test report can be found in Appendix B.
\subsection{Continuous Integration}
Our development flow is designed to fulfill the criteria of Continuous Integration (CI). The branching system is done, so there is a development branch a master branch as well as subbranches to the developer branch where new features are added. Automatic testing is added to the pipeline of the development branch to make sure the application is functional. Here there are two stages, building and testing. Due to time constraints only a few automatic tests have been added, but they prove that no functionality is broken. 

The CI testing is done by launching the server locally on our gitlab runner, and then running through the scripts written in webdriverIO. If the testing is succesful the pipeline will be marked as passed and the code commit is added to the development branch.

Before Development branch is pushed to the master branch which is then deployed, testing manually is done to ensure functionality of the application. The tests are the integration tests described in the RTM, but once these tests are made automatic they will be added to the development pipeline. Therefore this step will be excluded in future revisions. 

\subsection{Test Completeness}
The quality of the tests will be done according to the metrics in QA. The main metric we will be using is Branch coverage since we are almost exclusively using end 2 end testing. Branch coverage can be seen like a top down testing example, where we start in the starting page and look where we can go from each page, and test the functionality in this page. 

\subsection{Bug Tracking}
Bug tracking is done via gitlabs own issue boards. If a bug is found, the bug will be connected to the requirement that is tested, by labelling the requirement with the bug tag and writing down the bug in the issue as a comment. The test leader have continuous discussions with the development leader to ensure that the bugs are noticed and can provide a further description.

\clearpage